{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Colab 2, we will work to construct our own graph neural network using PyTorch Geometric (PyG) and then apply that model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on two different graph-based tasks: 1) node property prediction, predicting properties of single nodes and 2) graph property prediction, predicting properties of entire graphs or subgraphs.\n",
        "\n",
        "First, we will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
        "\n",
        "Then, we will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
        "\n",
        "Lastly, we will build our own graph neural network using PyTorch Geometric. We will then train and evaluate our model on the OGB node property prediction and graph property prediction tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKqVEbbMEzf"
      },
      "source": [
        "# Device\n",
        "You might need to use a GPU for this Colab to run quickly.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCK7krJdp4o8"
      },
      "source": [
        "# Setup\n",
        "As discussed in Colab 0, the installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vkP8pA1qBE5",
        "outputId": "41196a53-f9b4-4451-e134-eac288048c64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hujunhao/.conda/envs/GNN/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch has version 1.13.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr8hfxJ-qRg2",
        "outputId": "605e292c-1e06-469a-a844-c13d54fb3a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.12.1+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.0.tar.gz (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 14.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: torch-scatter\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.12.1+cu111.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.12.1+cu111.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# 1) PyTorch Geometric (Datasets and Data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "PyTorch Geometric has two classes for storing and/or transforming graphs into tensor format. One is `torch_geometric.datasets`, which contains a variety of common graph datasets. Another is `torch_geometric.data`, which provides the data handling of graphs in PyTorch tensors.\n",
        "\n",
        "In this section, we will learn how to use `torch_geometric.datasets` and `torch_geometric.data` together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "## PyG Datasets\n",
        "\n",
        "The `torch_geometric.datasets` class has many common graph datasets. Here we will explore its usage through one example dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT5qca3x6XpG",
        "outputId": "ecae7228-a03c-4bf2-eed9-ca4edadd2876"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Extracting enzymes/ENZYMES/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENZYMES(600)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  root = './enzymes'\n",
        "  name = 'ENZYMES'\n",
        "\n",
        "  # The ENZYMES dataset\n",
        "  pyg_dataset= TUDataset(root, name)\n",
        "\n",
        "  # You will find that there are 600 graphs in this dataset\n",
        "  print(pyg_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "## Question 1: What is the number of classes and number of features in the ENZYMES dataset? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iF_Kyqr_JbY",
        "outputId": "25d9176c-f4d8-4f8c-ae4b-66deeee87b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ENZYMES dataset has 6 classes\n",
            "ENZYMES dataset has 3 features\n"
          ]
        }
      ],
      "source": [
        "def get_num_classes(pyg_dataset):\n",
        "\n",
        "  num_classes = pyg_dataset.num_classes\n",
        "  return num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "\n",
        "  num_features = pyg_dataset.num_features\n",
        "  return num_features\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  num_classes = get_num_classes(pyg_dataset)\n",
        "  num_features = get_num_features(pyg_dataset)\n",
        "  print(\"{} dataset has {} classes\".format(name, num_classes))\n",
        "  print(\"{} dataset has {} features\".format(name, num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "## PyG Data\n",
        "\n",
        "Each PyG dataset stores a list of `torch_geometric.data.Data` objects, where each `torch_geometric.data.Data` object represents a graph. We can easily get the `Data` object by indexing into the dataset.\n",
        "\n",
        "For more information such as what is stored in the `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 2: What is the label of the graph with index 100 in the ENZYMES dataset? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIis9oTZAfs3",
        "outputId": "d28a3231-52ea-4186-ff34-987f593f0cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "Graph with index 100 has label tensor([4])\n"
          ]
        }
      ],
      "source": [
        "def get_graph_class(pyg_dataset, idx):\n",
        "  \n",
        "  label = pyg_dataset[idx].y\n",
        "  return label\n",
        "\n",
        "# Here pyg_dataset is a dataset for graph classification\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  graph_0 = pyg_dataset[0]\n",
        "  print(graph_0)\n",
        "  idx = 100\n",
        "  label = get_graph_class(pyg_dataset, idx)\n",
        "  print('Graph with index {} has label {}'.format(idx, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "## Question 3: How many edges does the graph with index 200 have? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5m2DOfhBtWv",
        "outputId": "7cabeb5e-963f-4d08-98b5-2b775968003d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph with index 200 has 53 edges\n"
          ]
        }
      ],
      "source": [
        "def get_graph_num_edges(pyg_dataset, idx):\n",
        "  # We assume the graph is undirected. You should not count an edge \n",
        "  # twice if the graph is undirected. \n",
        "  num_edges = pyg_dataset[idx].num_edges // 2\n",
        "  return num_edges\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  idx = 200\n",
        "  num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
        "  print('Graph with index {} has {} edges'.format(idx, num_edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# 2) Open Graph Benchmark (OGB)\n",
        "\n",
        "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can then be evaluated by using the OGB Evaluator in a unified manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Dataset and Data\n",
        "\n",
        "OGB also supports PyG dataset and data classes. Here we take a look on the `ogbn-arxiv` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpc6bTm3GF02",
        "outputId": "ffcb17d1-5764-46c0-9770-2206b0ef83a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:58<00:00,  1.38it/s]\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/arxiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3218.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n",
            "The ogbn-arxiv dataset has 1 graph\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  # Load the dataset and transform it to sparse tensor\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "  # Extract the graph\n",
        "  data = dataset[0]\n",
        "  print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0xZJKZI-n3"
      },
      "source": [
        "## Question 4: How many features are in the ogbn-arxiv graph? (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP844_nT2ZJl",
        "outputId": "f90e1ae0-9914-4fea-93ee-25e67dac8ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The graph has 128 features\n"
          ]
        }
      ],
      "source": [
        "def graph_num_features(data):\n",
        "\n",
        "  num_features = data.num_features\n",
        "  return num_features\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  num_features = graph_num_features(data)\n",
        "  print('The graph has {} features'.format(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "# 3) GNN: Node Property Prediction\n",
        "\n",
        "Specifically, we will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, we will work with PyG's built-in `GCNConv` layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "-DCtgcHpGIpd",
        "outputId": "6e716fca-1175-49d7-fd06-8084857aa4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.0\n"
          ]
        }
      ],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "0ibJ0ieoIwQM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  dataset_name = 'ogbn-arxiv'\n",
        "  dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                  transform=T.ToSparseTensor())\n",
        "  data = dataset[0]\n",
        "\n",
        "  # Make the adjacency matrix to symmetric\n",
        "  data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "  # If you use GPU, the device should be cuda\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  data = data.to(device)\n",
        "  split_idx = dataset.get_idx_split()\n",
        "  train_idx = split_idx['train'].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!\n",
        "\n",
        "Please follow the figure below to implement the `forward` function.\n",
        "\n",
        "\n",
        "![test](figs/cs224w-colab2-3.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_layers: The number of GCN layers. num_layers >= 2.\n",
        "            return_embeds: If True, skip classification layer and return node embeddings\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        super(GCN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
        "        self.convs.extend([GCNConv(hidden_dim, hidden_dim) for i in range(num_layers - 2)])\n",
        "        self.convs.extend([GCNConv(hidden_dim, output_dim)])\n",
        "        self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers-1)])\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "        self.dropout = dropout\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, adj_t)\n",
        "            if i < len(self.convs) - 1:\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        if not self.return_embeds:\n",
        "            x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(data.x, data.adj_t)\n",
        "    sliced_logits = logits[train_idx]\n",
        "    sliced_labels = data.y.squeeze(1)[train_idx]\n",
        "    loss = loss_fn(sliced_logits, sliced_labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "o7F46xkuLiOL"
      },
      "outputs": [],
      "source": [
        "# Please do not change the args\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  args = {\n",
        "      'device': device,\n",
        "      'num_layers': 3,\n",
        "      'hidden_dim': 256,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.01,\n",
        "      'epochs': 100,\n",
        "  }\n",
        "  args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model = GCN(data.num_features, args['hidden_dim'],\n",
        "              dataset.num_classes, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "  evaluator = Evaluator(name='ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "qd5O5cnPPdVF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Loss: 4.0571, Train: 28.18%, Valid: 30.28% Test: 27.15%\n",
            "Epoch: 02, Loss: 2.4017, Train: 23.68%, Valid: 16.21% Test: 16.67%\n",
            "Epoch: 03, Loss: 1.9662, Train: 28.11%, Valid: 24.44% Test: 29.32%\n",
            "Epoch: 04, Loss: 1.8023, Train: 36.93%, Valid: 29.61% Test: 34.05%\n",
            "Epoch: 05, Loss: 1.6890, Train: 41.92%, Valid: 43.11% Test: 46.16%\n",
            "Epoch: 06, Loss: 1.6091, Train: 40.90%, Valid: 41.95% Test: 44.04%\n",
            "Epoch: 07, Loss: 1.5229, Train: 38.59%, Valid: 37.80% Test: 40.59%\n",
            "Epoch: 08, Loss: 1.4597, Train: 37.44%, Valid: 34.75% Test: 37.89%\n",
            "Epoch: 09, Loss: 1.4244, Train: 37.95%, Valid: 34.34% Test: 37.34%\n",
            "Epoch: 10, Loss: 1.3878, Train: 39.78%, Valid: 36.38% Test: 38.57%\n",
            "Epoch: 11, Loss: 1.3414, Train: 42.90%, Valid: 40.64% Test: 42.97%\n",
            "Epoch: 12, Loss: 1.3228, Train: 45.37%, Valid: 43.08% Test: 45.70%\n",
            "Epoch: 13, Loss: 1.2972, Train: 48.12%, Valid: 46.17% Test: 48.35%\n",
            "Epoch: 14, Loss: 1.2795, Train: 51.19%, Valid: 49.82% Test: 51.26%\n",
            "Epoch: 15, Loss: 1.2623, Train: 53.46%, Valid: 52.31% Test: 52.81%\n",
            "Epoch: 16, Loss: 1.2400, Train: 54.94%, Valid: 54.20% Test: 54.84%\n",
            "Epoch: 17, Loss: 1.2269, Train: 56.00%, Valid: 55.72% Test: 57.00%\n",
            "Epoch: 18, Loss: 1.2090, Train: 57.12%, Valid: 57.39% Test: 58.93%\n",
            "Epoch: 19, Loss: 1.1944, Train: 57.98%, Valid: 58.51% Test: 59.80%\n",
            "Epoch: 20, Loss: 1.1863, Train: 58.46%, Valid: 58.51% Test: 59.72%\n",
            "Epoch: 21, Loss: 1.1719, Train: 58.52%, Valid: 58.22% Test: 59.41%\n",
            "Epoch: 22, Loss: 1.1631, Train: 58.42%, Valid: 57.69% Test: 59.27%\n",
            "Epoch: 23, Loss: 1.1536, Train: 58.67%, Valid: 57.75% Test: 59.64%\n",
            "Epoch: 24, Loss: 1.1456, Train: 59.86%, Valid: 59.28% Test: 61.20%\n",
            "Epoch: 25, Loss: 1.1368, Train: 61.79%, Valid: 61.73% Test: 63.06%\n",
            "Epoch: 26, Loss: 1.1320, Train: 63.27%, Valid: 63.24% Test: 63.74%\n",
            "Epoch: 27, Loss: 1.1209, Train: 64.42%, Valid: 64.14% Test: 64.32%\n",
            "Epoch: 28, Loss: 1.1157, Train: 65.29%, Valid: 65.24% Test: 65.52%\n",
            "Epoch: 29, Loss: 1.1076, Train: 65.96%, Valid: 65.98% Test: 66.44%\n",
            "Epoch: 30, Loss: 1.1072, Train: 66.48%, Valid: 66.46% Test: 66.97%\n",
            "Epoch: 31, Loss: 1.0977, Train: 66.81%, Valid: 66.77% Test: 66.97%\n",
            "Epoch: 32, Loss: 1.0886, Train: 67.12%, Valid: 66.89% Test: 66.82%\n",
            "Epoch: 33, Loss: 1.0846, Train: 67.39%, Valid: 67.08% Test: 66.94%\n",
            "Epoch: 34, Loss: 1.0796, Train: 67.58%, Valid: 67.35% Test: 67.56%\n",
            "Epoch: 35, Loss: 1.0779, Train: 67.71%, Valid: 67.56% Test: 67.96%\n",
            "Epoch: 36, Loss: 1.0712, Train: 67.96%, Valid: 67.86% Test: 68.18%\n",
            "Epoch: 37, Loss: 1.0633, Train: 68.29%, Valid: 68.11% Test: 68.26%\n",
            "Epoch: 38, Loss: 1.0602, Train: 68.53%, Valid: 68.18% Test: 68.18%\n",
            "Epoch: 39, Loss: 1.0531, Train: 68.73%, Valid: 68.09% Test: 67.80%\n",
            "Epoch: 40, Loss: 1.0519, Train: 68.84%, Valid: 68.27% Test: 68.00%\n",
            "Epoch: 41, Loss: 1.0466, Train: 68.94%, Valid: 68.42% Test: 68.55%\n",
            "Epoch: 42, Loss: 1.0464, Train: 69.07%, Valid: 68.77% Test: 68.87%\n",
            "Epoch: 43, Loss: 1.0395, Train: 69.48%, Valid: 69.25% Test: 69.19%\n",
            "Epoch: 44, Loss: 1.0353, Train: 69.96%, Valid: 69.65% Test: 69.14%\n",
            "Epoch: 45, Loss: 1.0335, Train: 70.33%, Valid: 69.62% Test: 68.92%\n",
            "Epoch: 46, Loss: 1.0286, Train: 70.40%, Valid: 69.52% Test: 68.61%\n",
            "Epoch: 47, Loss: 1.0213, Train: 70.34%, Valid: 69.54% Test: 68.85%\n",
            "Epoch: 48, Loss: 1.0200, Train: 70.31%, Valid: 69.39% Test: 68.87%\n",
            "Epoch: 49, Loss: 1.0227, Train: 70.51%, Valid: 69.65% Test: 69.00%\n",
            "Epoch: 50, Loss: 1.0135, Train: 70.80%, Valid: 69.66% Test: 68.69%\n",
            "Epoch: 51, Loss: 1.0121, Train: 70.97%, Valid: 69.59% Test: 68.42%\n",
            "Epoch: 52, Loss: 1.0080, Train: 71.03%, Valid: 70.00% Test: 69.10%\n",
            "Epoch: 53, Loss: 1.0073, Train: 71.05%, Valid: 70.20% Test: 69.62%\n",
            "Epoch: 54, Loss: 1.0034, Train: 71.27%, Valid: 70.46% Test: 69.63%\n",
            "Epoch: 55, Loss: 0.9989, Train: 71.43%, Valid: 70.49% Test: 69.29%\n",
            "Epoch: 56, Loss: 0.9983, Train: 71.57%, Valid: 70.32% Test: 68.84%\n",
            "Epoch: 57, Loss: 0.9945, Train: 71.73%, Valid: 70.58% Test: 69.31%\n",
            "Epoch: 58, Loss: 0.9915, Train: 71.78%, Valid: 70.88% Test: 70.07%\n",
            "Epoch: 59, Loss: 0.9902, Train: 71.86%, Valid: 70.97% Test: 70.17%\n",
            "Epoch: 60, Loss: 0.9859, Train: 71.94%, Valid: 70.96% Test: 69.91%\n",
            "Epoch: 61, Loss: 0.9896, Train: 71.93%, Valid: 70.89% Test: 69.75%\n",
            "Epoch: 62, Loss: 0.9846, Train: 71.94%, Valid: 70.74% Test: 69.74%\n",
            "Epoch: 63, Loss: 0.9829, Train: 71.84%, Valid: 70.84% Test: 70.25%\n",
            "Epoch: 64, Loss: 0.9804, Train: 72.08%, Valid: 71.07% Test: 70.52%\n",
            "Epoch: 65, Loss: 0.9743, Train: 72.29%, Valid: 71.27% Test: 70.29%\n",
            "Epoch: 66, Loss: 0.9731, Train: 72.38%, Valid: 71.05% Test: 69.63%\n",
            "Epoch: 67, Loss: 0.9702, Train: 72.38%, Valid: 71.14% Test: 69.76%\n",
            "Epoch: 68, Loss: 0.9716, Train: 72.27%, Valid: 71.24% Test: 70.48%\n",
            "Epoch: 69, Loss: 0.9646, Train: 72.41%, Valid: 71.33% Test: 70.75%\n",
            "Epoch: 70, Loss: 0.9659, Train: 72.47%, Valid: 71.45% Test: 70.75%\n",
            "Epoch: 71, Loss: 0.9655, Train: 72.51%, Valid: 71.44% Test: 70.63%\n",
            "Epoch: 72, Loss: 0.9620, Train: 72.57%, Valid: 71.43% Test: 70.74%\n",
            "Epoch: 73, Loss: 0.9592, Train: 72.60%, Valid: 71.50% Test: 70.66%\n",
            "Epoch: 74, Loss: 0.9572, Train: 72.76%, Valid: 71.47% Test: 70.50%\n",
            "Epoch: 75, Loss: 0.9557, Train: 72.79%, Valid: 71.43% Test: 70.33%\n",
            "Epoch: 76, Loss: 0.9548, Train: 72.73%, Valid: 71.44% Test: 70.72%\n",
            "Epoch: 77, Loss: 0.9525, Train: 72.55%, Valid: 71.34% Test: 70.94%\n",
            "Epoch: 78, Loss: 0.9483, Train: 72.81%, Valid: 71.48% Test: 70.92%\n",
            "Epoch: 79, Loss: 0.9504, Train: 73.04%, Valid: 71.55% Test: 70.18%\n",
            "Epoch: 80, Loss: 0.9464, Train: 72.90%, Valid: 70.79% Test: 68.78%\n",
            "Epoch: 81, Loss: 0.9467, Train: 72.99%, Valid: 71.30% Test: 69.71%\n",
            "Epoch: 82, Loss: 0.9440, Train: 72.89%, Valid: 71.36% Test: 70.51%\n",
            "Epoch: 83, Loss: 0.9416, Train: 72.88%, Valid: 71.24% Test: 70.63%\n",
            "Epoch: 84, Loss: 0.9403, Train: 73.06%, Valid: 71.41% Test: 70.51%\n",
            "Epoch: 85, Loss: 0.9396, Train: 73.16%, Valid: 71.29% Test: 70.06%\n",
            "Epoch: 86, Loss: 0.9386, Train: 73.21%, Valid: 71.55% Test: 70.66%\n",
            "Epoch: 87, Loss: 0.9378, Train: 73.13%, Valid: 71.79% Test: 71.30%\n",
            "Epoch: 88, Loss: 0.9337, Train: 73.19%, Valid: 71.92% Test: 71.20%\n",
            "Epoch: 89, Loss: 0.9315, Train: 73.19%, Valid: 71.67% Test: 70.73%\n",
            "Epoch: 90, Loss: 0.9303, Train: 73.24%, Valid: 71.49% Test: 70.67%\n",
            "Epoch: 91, Loss: 0.9277, Train: 73.31%, Valid: 71.59% Test: 70.66%\n",
            "Epoch: 92, Loss: 0.9250, Train: 73.45%, Valid: 71.70% Test: 70.34%\n",
            "Epoch: 93, Loss: 0.9265, Train: 73.58%, Valid: 71.92% Test: 70.76%\n",
            "Epoch: 94, Loss: 0.9262, Train: 73.56%, Valid: 71.99% Test: 71.19%\n",
            "Epoch: 95, Loss: 0.9249, Train: 73.61%, Valid: 71.93% Test: 70.89%\n",
            "Epoch: 96, Loss: 0.9223, Train: 73.51%, Valid: 71.85% Test: 70.81%\n",
            "Epoch: 97, Loss: 0.9183, Train: 73.47%, Valid: 71.87% Test: 71.09%\n",
            "Epoch: 98, Loss: 0.9200, Train: 73.50%, Valid: 71.85% Test: 70.96%\n",
            "Epoch: 99, Loss: 0.9188, Train: 73.62%, Valid: 72.02% Test: 71.27%\n",
            "Epoch: 100, Loss: 0.9140, Train: 73.65%, Valid: 71.88% Test: 70.96%\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # reset the parameters to initial random value\n",
        "  model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  loss_fn = F.nll_loss\n",
        "\n",
        "  best_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQtt-EKA8P4r"
      },
      "source": [
        "## Question 5: What are your `best_model` validation and test accuracies?(20 points)\n",
        "\n",
        "Run the cell below to see the results of your best of model and save your model's predictions to a file named *ogbn-arxiv_node.csv*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "EqcextqOL2FX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 73.62%, Valid: 72.02% Test: 71.27%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pOD6y80TyI"
      },
      "source": [
        "# 4) GNN: Graph Property Prediction\n",
        "\n",
        "In this section we will create a graph neural network for graph property prediction (graph classification).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRg5VOEdQTa4"
      },
      "source": [
        "## Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "LXb-O5QUIgTH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/hiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:00<00:00, 66097.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41127/41127 [00:01<00:00, 38556.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n",
            "Device: cuda\n",
            "Task type: binary classification\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # Load the dataset \n",
        "  dataset = PygGraphPropPredDataset(name='ogbg-molhiv')\n",
        "\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  print('Device: {}'.format(device))\n",
        "\n",
        "  split_idx = dataset.get_idx_split()\n",
        "\n",
        "  # Check task type\n",
        "  print('Task type: {}'.format(dataset.task_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "7cHHbgW1c5hi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hujunhao/.conda/envs/GNN/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
        "  valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
        "  test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "AYrSnOj0Y4DK"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # Please do not change the args\n",
        "  args = {\n",
        "      'device': device,\n",
        "      'num_layers': 5,\n",
        "      'hidden_dim': 256,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.001,\n",
        "      'epochs': 30,\n",
        "  }\n",
        "  args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WLhguSTeazy"
      },
      "source": [
        "## Graph Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05Z14TRYPGn"
      },
      "source": [
        "### Graph Mini-Batching\n",
        "Before diving into the actual model, we introduce the concept of mini-batching with graphs. In order to parallelize the processing of a mini-batch of graphs, PyG combines the graphs into a single disconnected graph data object (*torch_geometric.data.Batch*). *torch_geometric.data.Batch* inherits from *torch_geometric.data.Data* (introduced earlier) and contains an additional attribute called `batch`. \n",
        "\n",
        "The `batch` attribute is a vector mapping each node to the index of its corresponding graph within the mini-batch:\n",
        "\n",
        "    batch = [0, ..., 0, 1, ..., n - 2, n - 1, ..., n - 1]\n",
        "\n",
        "This attribute is crucial for associating which graph each node belongs to and can be used to e.g. average the node embeddings for each graph individually to compute graph level embeddings. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcic9NNU3nGK"
      },
      "source": [
        "### Implemention\n",
        "\n",
        "We will reuse the existing GCN model to generate `node_embeddings` and then use  `Global Pooling`. Remeber that the `batch` attribute will be essential for performining Global Pooling over our mini-batch of graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Kq3zyjeZ22"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
        "\n",
        "### GCN to predict graph property\n",
        "class GCN_Graph(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super(GCN_Graph, self).__init__()\n",
        "\n",
        "        # Load encoders for Atoms in molecule graphs\n",
        "        self.node_encoder = AtomEncoder(hidden_dim)\n",
        "\n",
        "        # Node embedding model\n",
        "        # Note that the input_dim and output_dim are set to hidden_dim\n",
        "        self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
        "            hidden_dim, num_layers, dropout, return_embeds=True)\n",
        "\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Output layer\n",
        "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      self.gnn_node.reset_parameters()\n",
        "      self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        # Extract important attributes of our mini-batch\n",
        "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
        "        embed = self.node_encoder(x)\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Construct node embeddings using existing GCN model\n",
        "        ## 2. Use the global pooling layer to aggregate features for each individual graph\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
        "        ## 3. Use a linear layer to predict each graph's property\n",
        "        ## (~3 lines of code)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJjnGuMSbjX0"
      },
      "outputs": [],
      "source": [
        "def train(model, device, data_loader, optimizer, loss_fn):\n",
        "    # TODO: Implement a function that trains your model by \n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "          pass\n",
        "      else:\n",
        "        ## ignore nan targets (unlabeled) when computing training loss.\n",
        "        is_labeled = batch.y == batch.y\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Zero grad the optimizer\n",
        "        ## 2. Feed the data into the model\n",
        "        ## 3. Use `is_labeled` mask to filter output and labels\n",
        "        ## 4. You may need to change the type of label to torch.float32\n",
        "        ## 5. Feed the output and label to the loss_fn\n",
        "        ## (~3 lines of code)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztPHXq_Gzn7U"
      },
      "outputs": [],
      "source": [
        "# The evaluation function\n",
        "def eval(model, device, loader, evaluator, save_model_results=False, save_file=None):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch)\n",
        "\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    if save_model_results:\n",
        "        print (\"Saving Model Predictions\")\n",
        "        \n",
        "        # Create a pandas dataframe with a two columns\n",
        "        # y_pred | y_true\n",
        "        data = {}\n",
        "        data['y_pred'] = y_pred.reshape(-1)\n",
        "        data['y_true'] = y_true.reshape(-1)\n",
        "\n",
        "        df = pd.DataFrame(data=data)\n",
        "        # Save to csv\n",
        "        df.to_csv('ogbg-molhiv_graph_' + save_file + '.csv', sep=',', index=False)\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR1wQ4hMZeMw"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model = GCN_Graph(args['hidden_dim'],\n",
        "              dataset.num_tasks, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "  evaluator = Evaluator(name='ogbg-molhiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJGTNZiuZy0A"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  best_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    print('Training...')\n",
        "    loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "    print('Evaluating...')\n",
        "    train_result = eval(model, device, train_loader, evaluator)\n",
        "    val_result = eval(model, device, valid_loader, evaluator)\n",
        "    test_result = eval(model, device, test_loader, evaluator)\n",
        "\n",
        "    train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I17-Qso_n88"
      },
      "source": [
        "## Question 6: What are your `best_model` validation and test ROC-AUC scores? (20 points)\n",
        "\n",
        "Run the cell below to see the results of your best of model and save your model's predictions over the validation and test datasets. The resulting files are named *ogbn-arxiv_graph_valid.csv* and *ogbn-arxiv_graph_test.csv*. \n",
        "\n",
        "Again, you can view these files by clicking on the *Folder* icon on the left side pannel. As in Colab 1, when you sumbit your assignment, you will have to download these files and attatch them to your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq5QaG21dOOO"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "  valid_acc = eval(best_model, device, valid_loader, evaluator, save_model_results=True, save_file=\"valid\")[dataset.eval_metric]\n",
        "  test_acc  = eval(best_model, device, test_loader, evaluator, save_model_results=True, save_file=\"test\")[dataset.eval_metric]\n",
        "\n",
        "  print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5 ('GNN')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "26e6efd3402547c218c2721ab118aebc75d50eb2bba058d3de4ea71b8f32fd6b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
